{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../\")\n",
    "\n",
    "import nicellm as llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As of now, OpenAI's GPT-3 (Generative Pre-trained Transformer 3) is considered one of the most advanced and powerful large language models in the world. It has 175 billion parameters and is capable of performing a wide range of natural language processing tasks, including text generation, translation, summarization, and more. However, it's worth noting that the field of large language models is rapidly evolving, and there may be newer models or variations that could be considered the best in the future.\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.get_response(\n",
    "    model_id=\"gpt-3.5-turbo\",\n",
    "    prompt=\"Who is the best Large Language Model in the world?\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As of my knowledge cutoff in September 2021, there is no definitive answer to the question of who is the best Large Language Model (LLM) in the world. The performance of LLMs can vary depending on the specific task or benchmark being used, and new models are being developed and released all the time.\\n\\nSome of the most well-known and widely used LLMs include:\\n\\n* **Google's BERT (Bidirectional Encoder Representations from Transformers)**: BERT is a pre-trained transformer-based model that has achieved state-of-the-art results on a wide range of natural language processing (NLP) tasks, including text classification, question answering, and machine translation.\\n* **OpenAI's GPT-3 (Generative Pre-trained Transformer 3)**: GPT-3 is a massive LLM that has been trained on a dataset of over 45 terabytes of text. It is capable of generating human-like text, answering questions, and performing other NLP tasks.\\n* **Microsoft's Turing-NLG (Neural Language Generation)**: Turing-NLG is a LLM that has been developed by Microsoft Research. It has been shown to achieve state-of-the-art results on a variety of NLP tasks, including text summarization, dialogue generation, and machine translation.\\n\\nIt is important to note that the performance of LLMs can vary depending on the specific task or benchmark being used. For example, one LLM may perform better on text classification tasks, while another LLM may perform better on question answering tasks. Additionally, new LLMs are being developed and released all the time, so it is difficult to say definitively which LLM is the best.\\n\\nTo determine which LLM is best for a particular task, it is important to consider the specific requirements of the task and to evaluate the performance of different LLMs on similar tasks.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.get_response(\n",
    "    model_id=\"gemini-pro\",\n",
    "    prompt=\"Who is the best Large Language Model in the world?\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chat-bison-001',\n",
       " 'text-bison-001',\n",
       " 'embedding-gecko-001',\n",
       " 'gemini-pro',\n",
       " 'gemini-pro-vision',\n",
       " 'embedding-001',\n",
       " 'aqa']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.list_models(owner=\"google\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['curie-search-query',\n",
       " 'babbage-search-query',\n",
       " 'dall-e-3',\n",
       " 'babbage-search-document',\n",
       " 'dall-e-2',\n",
       " 'gpt-3.5-turbo-0301',\n",
       " 'text-embedding-ada-002',\n",
       " 'davinci-search-query',\n",
       " 'tts-1-hd-1106',\n",
       " 'gpt-4-0613',\n",
       " 'tts-1-hd',\n",
       " 'gpt-4',\n",
       " 'ada-search-document',\n",
       " 'ada-code-search-code',\n",
       " 'davinci-002',\n",
       " 'babbage-002',\n",
       " 'gpt-4-vision-preview',\n",
       " 'davinci-search-document',\n",
       " 'curie-search-document',\n",
       " 'whisper-1',\n",
       " 'ada-search-query',\n",
       " 'ada-code-search-text',\n",
       " 'gpt-3.5-turbo-1106',\n",
       " 'gpt-3.5-turbo-16k-0613',\n",
       " 'gpt-3.5-turbo-16k',\n",
       " 'gpt-3.5-turbo',\n",
       " 'gpt-3.5-turbo-0613',\n",
       " 'gpt-3.5-turbo-instruct',\n",
       " 'gpt-3.5-turbo-instruct-0914',\n",
       " 'gpt-4-1106-preview',\n",
       " 'tts-1-1106',\n",
       " 'tts-1']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.list_models(owner=\"openai\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
